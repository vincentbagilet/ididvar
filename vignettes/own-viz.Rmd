---
title: "Own visualizations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Own visualizations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r load_packages, message=FALSE}
library(ididvar)
library(tigris)
library(dplyr)
library(ggplot2)
```

This vignette **provides code that can easily be copied/pasted to make your own customized plots**, going beyond the quick but less customizable functions provided in the package.

This package exists to facilitate the computation and analysis of identifying variation weights. It therefore includes a set of functions to quickly visualize those weights. These functions have names starting by `idid_viz`. However, one may want to *customize* their graphs or build their own. 

## Workflow to plot identifying variation weights

The workflow to make a plot from scratch is straightforward: 

1. Add a `weight` variable to the data set. Optionally, one can add a `contrib` dummy. For that we need to determine the contribution threshold. Here I use the `idid_contrib_threshold` to find a weight threshold below which removing observations does not change the point estimate or the standard error of the estimate of interest by more than a given proportion.
2. Compute the weights at a group level of interest (here state level) by summing weights of observations in this group.
3. Plot them

## Preparation

Let's assume we have already ran the regression of interest.

```{r run_reg}
data_state_clean <- state.x77 |> 
  as_tibble() |> 
  mutate(state_name = rownames(state.x77), .before = 1) 

reg <- data_state_clean |> 
  lm(formula = Illiteracy ~  Income + Population + `Life Exp` + Frost)
```

We can then add a `weight` variable to our dataset, along with a `contrib` dummy. 

Let's assume that we want to aggregate the data at the state level. We compute our weight and mean contribution at that level. Note that this step is useless here as our data is already at the state level. Next, we compare the weights to the average weight (the inverse of the number of observations) and take its log10. This step is crucial to then be able to apply the `scale_fill_idid` color scale and to build a graph with meaningful weights.

```{r agg_weights, fig.asp=0.8}
data_state_weights <- data_state_clean |> 
  mutate(
    weight = idid_weights(reg, "Income"),
    contrib = weight > idid_contrib_threshold(reg, "Income")
  ) |> 
  group_by(state_name) |> 
  summarise(
    weight = sum(weight, na.rm = TRUE), 
    contrib = mean(contrib, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  mutate(weight_log = log10(weight * length(weight))) 
```

## Bar plots {.tabset}

### Own weight graph

We will first make bar plots. For readibility and to produce a better graph, we order the states by weight.

```{r weight_bar_plot, echo=TRUE}
data_state_weights |> 
  mutate(state_name = forcats::fct_reorder(state_name, weight)) |> 
  ggplot(aes(x = weight, y = state_name)) + 
  geom_col(fill = "#300D49") +
  theme_idid() +
  labs(
    title = "Distribution of Identifying Variation Weights",
    x = "Weight",
    y = NULL
  ) 
```

### ididvar weight equivalent

```{r idid_weight_bar_plot, echo=TRUE}
idid_viz_weights(reg, "Income", var_y = state_name, order = "y") +
  labs(
    title = "Distribution of Identifying Variation Weights",
    x = "Weight",
    y = NULL
  ) 
```

### Own contribution graph

```{r contrib_bar_plot, echo=TRUE}
data_state_weights |> 
  # mutate(state_name = forcats::fct_reorder(state_name, contrib)) |> 
  ggplot(aes(x = contrib, y = state_name)) + 
  geom_col(fill = "#300D49") +
  theme_idid() +
  labs(
    title = "Distribution of Identifying Variation Weights",
    x = "Weight",
    y = NULL
  ) 
```

### ididvar contribution equivalent

```{r idid_contrib_bar_plot, echo=TRUE}
idid_viz_weights(reg, "Income", var_y = state_name, order = "y") +
  labs(
    title = "Distribution of Identifying Variation Weights",
    x = "Weight",
    y = NULL
  ) 
```

## Heatmaps

## Maps

To make a map, we need an `sf` object:

```{r states_sf}
states_sf <- 
  tigris::states(cb = TRUE, resolution = "20m", progress_bar = FALSE) |>
  tigris::shift_geometry() |> 
  rename(state_name = NAME)
```

We can then merge the data to the shape file and make the graphs of interest.

```{r map_weights, fig.asp=0.8}
data_sf <- states_sf |> 
  full_join(data_state_weights, by = join_by(state_name)) 

data_sf |> 
  ggplot() +
  geom_sf(aes(fill = weight_log), color = "white", linewidth = 0.1) +
  theme_idid() +
  scale_fill_idid() +
  labs(
    title = "Distribution of Identifying Variation Weights",
    fill = "Weight, compared to the average weight"
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank())

data_sf |> 
  ggplot() +
  geom_sf(aes(fill = contrib), color = "white", linewidth = 0.1) +
  theme_idid() +
  scale_fill_gradient(low = "#FBE2C5", high = "#300D49") +
  labs(
    title = "Distribution of contributing observations per state",
    fill = "Share of contributing observations",
    caption = "In this particular example, there is only one observation per state.\nThe shares are threfore 0 or 1."
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank())
``` 

The maps produced in this document are not particularly instructive since the regression model is paritcularly naive.

<!-- # Why keeping the idid scale -->

<!-- I argue for keeping the idid scale -->

## Relationship between x and y

One may want to visualize the weights as well as the relationship between the dependent variable and the independent variable of interest. To do so, they can use `idid_viz_bivar` or the following code:

```{r xy_rel}
#| fig-asp: 0.8

data_partial <- 
  tibble(
    illiteracy_per = idid_partial_out(reg, "Illiteracy"),
    income_per = idid_partial_out(reg, "Income"),
    weight = (income_per - mean(income_per))^2
  ) |> 
  mutate(
    weight = weight/sum(weight),
    logweight = log10(weight * length(weight))
  )

data_partial |>
  ggplot(aes(x = income_per, y = illiteracy_per, color = logweight)) +
  geom_point(size = 2) +
  geom_rug(linewidth = 0.8) +
  # ggExtra::ggMarginal(type = "histogram") +
  geom_smooth(
    method = "lm", 
    color = idid_colors_table[["base"]],
    fill = idid_colors_table[["base"]], 
    alpha = 0.1) +
  theme_idid() +
  scale_color_idid() +
  labs(
    title = "Relationship between illiteracy and income",
    subtitle = "After partialling out controls",
    color = "Weight, as compared to the average weight",
    x = "Income (residualized)",
    y = "Illiteracy (residualized)"
  )
```

Such a mapping might be less legible when the sample size is large. In such instances, transparency or a bin or hex map can be useful tools to represent large numbers of observations.



